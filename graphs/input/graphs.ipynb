{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy3"
      ],
      "metadata": {
        "id": "5Y683h5Q6X1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyvis"
      ],
      "metadata": {
        "id": "f1BN0xpiEFM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import nltk\n",
        "import pymorphy3\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from pyvis.network import Network\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "euWeOew1Q2PE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "vGie5QatTVSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Подготовка данных"
      ],
      "metadata": {
        "id": "KPMd7wVHre_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# перед этим загрузить файл all_data.csv\n",
        "df = pd.read_csv('all_data.csv', encoding='utf-8')\n",
        "target_columns = [\n",
        "    'Запишите, пожалуйста, вступление, которое вы слышали/использовали чаще всего',\n",
        "    'Запишите, пожалуйста, другой вариант вступления, который вы слышали',\n",
        "    'Запишите, пожалуйста, ещё один вариант, если вы слышали какую-то третью вариацию вступления'\n",
        "]\n",
        "df_filtered = df[target_columns].copy()\n",
        "df_filtered.columns = ['intro1', 'intro2', 'intro3']\n",
        "all_entries = pd.concat([\n",
        "    df_filtered['intro1'],\n",
        "    df_filtered['intro2'],\n",
        "    df_filtered['intro3']\n",
        "], ignore_index=True)\n",
        "all_entries_clean = all_entries.dropna()\n",
        "all_entries_clean = all_entries_clean.astype(str).str.strip()"
      ],
      "metadata": {
        "id": "uGu6xo7CxQdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# файл с непочищенными вступлениями\n",
        "all_entries_clean.to_csv('intro_uncleaned.csv', index=False, encoding='utf-8')"
      ],
      "metadata": {
        "id": "Vl31wJ3eROyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_entries_clean = all_entries_clean.str.lower().str.replace(r'[^\\w\\s]', '', regex=True)\n",
        "df_result = pd.DataFrame({'intro': all_entries_clean})\n",
        "df_result = df_result[df_result['intro'] != '']\n",
        "df_sorted_asc = df_result.sort_values('intro', ascending=True)"
      ],
      "metadata": {
        "id": "JCgbTXP6x791"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# файл без знаков препинания, отсортированный\n",
        "df_sorted_asc.to_csv('intro.csv', index=False, encoding='utf-8')"
      ],
      "metadata": {
        "id": "EVZwSW20RdPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_result.head(10)"
      ],
      "metadata": {
        "id": "iqxBqbVAuTrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Лемматизация и нахождение пар"
      ],
      "metadata": {
        "id": "DZ_2Lac27DcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_and_count_pairs(input_file, output_file):\n",
        "    morph = pymorphy3.MorphAnalyzer()\n",
        "    pairs_counter = Counter()\n",
        "\n",
        "    with open(input_file, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            words = [word.strip() for word in line.strip().split() if word.strip()]\n",
        "            lemmas = []\n",
        "            for word in words:\n",
        "                parsed = morph.parse(word.lower())[0]\n",
        "                lemmas.append(parsed.normal_form)\n",
        "            for i in range(len(lemmas) - 1):\n",
        "                pair = (lemmas[i], lemmas[i + 1])\n",
        "                pairs_counter[pair] += 1\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(['word1', 'word2', 'count'])\n",
        "        sorted_pairs = sorted(pairs_counter.items(), key=lambda x: x[1], reverse=True)\n",
        "        for (word1, word2), count in sorted_pairs:\n",
        "            writer.writerow([word1, word2, count])"
      ],
      "metadata": {
        "id": "3T1O11VX7O0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# перед этим загрузить просмотренный вручную файл intro_cleaned.csv\n",
        "if __name__ == '__main__':\n",
        "    lemmatize_and_count_pairs('intro_cleaned.csv', 'pairs_lemmatized.csv')"
      ],
      "metadata": {
        "id": "oO9oYyhF7l_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Построение графов"
      ],
      "metadata": {
        "id": "xglMM16bS8Ng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# перед этим загрузить файл с исправленными леммами pairs_lemmatized_cleaned.csv\n",
        "# строим граф на всех данных\n",
        "def network_from_all_pairs(pairs_file, output_html='intro_network_all.html'):\n",
        "    df = pd.read_csv(pairs_file)\n",
        "    net = Network(notebook=True, height='750px', width='100%', bgcolor='#222222', font_color='white')\n",
        "    for _, row in df.iterrows():\n",
        "        net.add_node(row['word1'], title=row['word1'], size=15)\n",
        "        net.add_node(row['word2'], title=row['word2'], size=15)\n",
        "        net.add_edge(row['word1'], row['word2'], value=row['count'], title=f'Частота: {row['count']}')\n",
        "    net.force_atlas_2based(gravity=-100, central_gravity=0.01, spring_length=100)\n",
        "    net.show(output_html)"
      ],
      "metadata": {
        "id": "R2cA_coC_Gx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network_from_all_pairs('pairs_lemmatized_cleaned.csv')"
      ],
      "metadata": {
        "id": "k-vRAdLnSMlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# строим граф с ограничением частотности >= 2\n",
        "def network_from_pairs(pairs_file, output_html='intro_network_2andmore.html'):\n",
        "    df = pd.read_csv(pairs_file)\n",
        "    df = df[df['count'] >= 2]\n",
        "    net = Network(notebook=True, height='750px', width='100%', bgcolor='#222222', font_color='white')\n",
        "    for _, row in df.iterrows():\n",
        "        net.add_node(row['word1'], title=row['word1'], size=15)\n",
        "        net.add_node(row['word2'], title=row['word2'], size=15)\n",
        "        net.add_edge(row['word1'], row['word2'], value=row['count'], title=f\"Частота: {row['count']}\")\n",
        "    net.force_atlas_2based(gravity=-100, central_gravity=0.01, spring_length=100)\n",
        "    net.show(output_html)"
      ],
      "metadata": {
        "id": "pYlv3b5I8-hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network_from_pairs('pairs_lemmatized_cleaned.csv')"
      ],
      "metadata": {
        "id": "OkUZfl749ZiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# строим граф без стоп-слов с ограничением частотности\n",
        "def clean_stopwords(input_file, output_file):\n",
        "    stop = set(stopwords.words('russian'))\n",
        "    clean_pairs = []\n",
        "    with open(input_file, 'r', encoding='utf-8') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        for row in reader:\n",
        "            word1 = row['word1']\n",
        "            word2 = row['word2']\n",
        "            count = int(row['count'])\n",
        "            # Пропускаем если оба слова не стоп-слова\n",
        "            if word1 not in stop and word2 not in stop:\n",
        "                clean_pairs.append((word1, word2, count))\n",
        "    clean_pairs.sort(key=lambda x: x[2], reverse=True)\n",
        "    with open(output_file, 'w', encoding='utf-8', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(['word1', 'word2', 'count'])\n",
        "        writer.writerows(clean_pairs)\n",
        "    return clean_pairs"
      ],
      "metadata": {
        "id": "F6g3f6_i_K1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    clean_pairs = clean_stopwords('pairs_lemmatized_cleaned.csv', 'pairs_cleaned.csv')"
      ],
      "metadata": {
        "id": "yYHkJBhZTs7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def network_from_pairs_stop(pairs_file, output_html='intro_network_stop.html'):\n",
        "    df = pd.read_csv(pairs_file)\n",
        "    df = df[df['count'] >= 2]\n",
        "    net = Network(notebook=True, height='750px', width='100%', bgcolor='#222222', font_color='white')\n",
        "    for _, row in df.iterrows():\n",
        "        net.add_node(row['word1'], title=row['word1'], size=15)\n",
        "        net.add_node(row['word2'], title=row['word2'], size=15)\n",
        "        net.add_edge(row['word1'], row['word2'], value=row['count'], title=f'Частота: {row['count']}')\n",
        "    net.force_atlas_2based(gravity=-100, central_gravity=0.01, spring_length=100)\n",
        "    net.show(output_html)"
      ],
      "metadata": {
        "id": "SDrQ9Yt3Tyjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network_from_pairs_stop('pairs_cleaned.csv')"
      ],
      "metadata": {
        "id": "Aiza-qVBTyjo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}